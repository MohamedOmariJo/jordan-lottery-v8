"""
=============================================================================
ğŸ§  Ù†Ø¸Ø§Ù… Machine Learning Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªÙ†Ø¨Ø¤
=============================================================================
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
from collections import Counter
import joblib
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score
import warnings
warnings.filterwarnings('ignore')

from config.settings import Config
from utils.logger import logger
from utils.performance import PerformanceBenchmark

class LotteryPredictor:
    """Ù†Ø¸Ø§Ù… ØªÙ†Ø¨Ø¤ Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Machine Learning"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.is_trained = False
        self.benchmark = PerformanceBenchmark()
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        self._initialize_models()
    
    def _initialize_models(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ ML Ù…Ø®ØªÙ„ÙØ©"""
        # 1. Random Forest (Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¹Ø§Ù…)
        self.models['random_forest'] = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        # 2. Gradient Boosting (Ù„Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¯Ù‚ÙŠÙ‚)
        self.models['gradient_boosting'] = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42
        )
        
        # Scalers
        for model_name in self.models:
            self.scalers[model_name] = StandardScaler()
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Ø¥Ø¹Ø¯Ø§Ø¯ features Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        operation_id = logger.start_operation('feature_preparation', {
            'total_draws': len(df),
            'models_count': len(self.models)
        })
        
        features_list = []
        labels_list = []
        
        try:
            for i in range(len(df) - 2):  # Ù†Ø­ØªØ§Ø¬ Ø³Ø­Ø¨ÙŠÙ† Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„
                current = df.iloc[i]['numbers']
                next_draw = df.iloc[i + 1]['numbers']
                future_draw = df.iloc[i + 2]['numbers']
                
                # Basic features
                basic_features = [
                    *current,  # Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ©
                    sum(current),  # Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹
                    sum(1 for n in current if n % 2),  # Ø¹Ø¯Ø¯ Ø§Ù„ÙØ±Ø¯ÙŠ
                    sum(1 for i in range(len(current)-1) if current[i+1] - current[i] == 1),  # Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ§Øª
                    current[-1] - current[0],  # Ø¹Ø±Ø¶ Ø§Ù„Ù†Ø·Ø§Ù‚
                    np.mean(current),  # Ø§Ù„Ù…ØªÙˆØ³Ø·
                    np.std(current)  # Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ
                ]
                
                # Statistical features
                freq_counter = Counter(list(chain.from_iterable(df.iloc[:i+1]['numbers'])))
                statistical_features = [
                    np.mean([freq_counter.get(n, 0) for n in current]),  # Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙƒØ±Ø§Ø±
                    np.std([freq_counter.get(n, 0) for n in current]),  # Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„ØªÙƒØ±Ø§Ø±
                    len(set(current) & set(next_draw)),  # ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„ØªØ§Ù„ÙŠ
                ]
                
                # Pattern features
                pattern_features = [
                    len(set([n % 10 for n in current])),  # ØªÙ†ÙˆØ¹ Ø§Ù„Ø¢Ø­Ø§Ø¯
                    sum(1 for n in current if self._is_prime(n)),  # Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
                    self._calculate_balance(current)  # Ø¯Ø±Ø¬Ø© Ø§Ù„ØªÙˆØ§Ø²Ù†
                ]
                
                # Combine all features
                feature_vector = basic_features + statistical_features + pattern_features
                features_list.append(feature_vector)
                
                # Label: Ù‡Ù„ ÙŠØ¸Ù‡Ø± Ø§Ù„Ø±Ù‚Ù… ÙÙŠ Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØŸ
                # Ù†Ø­Ù† Ù†ØªÙˆÙ‚Ø¹ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¸Ù‡ÙˆØ± ÙƒÙ„ Ø±Ù‚Ù…
                for num in range(1, 33):
                    label = 1 if num in future_draw else 0
                    labels_list.append(label)
            
            features_array = np.array(features_list)
            labels_array = np.array(labels_list)
            
            logger.end_operation(operation_id, 'completed', {
                'features_shape': features_array.shape,
                'labels_shape': labels_array.shape,
                'feature_count': features_array.shape[1]
            })
            
            return features_array, labels_array
            
        except Exception as e:
            logger.end_operation(operation_id, 'failed', {'error': str(e)})
            raise
    
    def train(self, df: pd.DataFrame, model_name: str = 'random_forest'):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø¯Ø¯"""
        operation_id = logger.start_operation('model_training', {
            'model': model_name,
            'data_size': len(df)
        })
        
        try:
            self.benchmark.start_monitoring(f'train_{model_name}')
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X, y = self.prepare_features(df)
            
            if X.shape[0] < 10:
                raise ValueError(f"Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨: {X.shape[0]} Ø¹ÙŠÙ†Ø© ÙÙ‚Ø·")
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            X_train_scaled = self.scalers[model_name].fit_transform(X_train)
            X_test_scaled = self.scalers[model_name].transform(X_test)
            
            # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            model = self.models[model_name]
            model.fit(X_train_scaled, y_train)
            
            # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            y_pred = model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            
            # Cross-validation
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
            
            # Ø­Ø³Ø§Ø¨ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª
            if hasattr(model, 'feature_importances_'):
                self.feature_importance[model_name] = model.feature_importances_
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self._save_model(model_name)
            
            metrics = self.benchmark.stop_monitoring(f'train_{model_name}')
            
            logger.end_operation(operation_id, 'completed', {
                'accuracy': round(accuracy, 4),
                'precision': round(precision, 4),
                'recall': round(recall, 4),
                'cv_mean': round(cv_scores.mean(), 4),
                'cv_std': round(cv_scores.std(), 4),
                'training_samples': X_train.shape[0],
                'testing_samples': X_test.shape[0],
                **metrics
            })
            
            logger.log_prediction(
                model_name=model_name,
                accuracy=accuracy,
                confidence=precision,
                features_used=[f'feature_{i}' for i in range(X.shape[1])]
            )
            
            self.is_trained = True
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'cv_scores': cv_scores.tolist(),
                'feature_importance': self.feature_importance.get(model_name, []).tolist()
            }
            
        except Exception as e:
            logger.end_operation(operation_id, 'failed', {'error': str(e)})
            raise
    
    def predict(self, current_numbers: List[int], df: pd.DataFrame, 
                top_n: int = 10, model_name: str = 'random_forest') -> List[Tuple[int, float]]:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªØ§Ù„ÙŠØ©"""
        if not self.is_trained or model_name not in self.models:
            raise ValueError(f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…Ø¯Ø±Ø¨")
        
        operation_id = logger.start_operation('prediction', {
            'model': model_name,
            'current_numbers': current_numbers,
            'top_n': top_n
        })
        
        try:
            self.benchmark.start_monitoring(f'predict_{model_name}')
            
            # ØªØ­Ø¶ÙŠØ± features Ù„Ù„Ø±Ù‚Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ
            feature_vector = self._prepare_single_features(current_numbers, df)
            
            # ØªØ·Ø¨ÙŠØ¹
            scaled_features = self.scalers[model_name].transform([feature_vector])
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„ÙƒÙ„ Ø±Ù‚Ù… Ù…Ù…ÙƒÙ†
            predictions = []
            model = self.models[model_name]
            
            for num in range(1, 33):
                if num in current_numbers:
                    continue
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ø¹ Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
                test_features = scaled_features.copy()
                
                # Ø§Ù„ØªÙ†Ø¨Ø¤
                prob = model.predict_proba(test_features)[0][1]  # Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø¸Ù‡ÙˆØ±
                predictions.append((num, prob))
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©
            predictions.sort(key=lambda x: x[1], reverse=True)
            top_predictions = predictions[:top_n]
            
            metrics = self.benchmark.stop_monitoring(f'predict_{model_name}')
            
            logger.end_operation(operation_id, 'completed', {
                'top_predictions': top_predictions,
                'highest_probability': top_predictions[0][1] if top_predictions else 0,
                **metrics
            })
            
            return top_predictions
            
        except Exception as e:
            logger.end_operation(operation_id, 'failed', {'error': str(e)})
            raise
    
    def _prepare_single_features(self, numbers: List[int], df: pd.DataFrame) -> np.ndarray:
        """ØªØ­Ø¶ÙŠØ± features Ù„Ø³Ø­Ø¨ ÙˆØ§Ø­Ø¯"""
        # Basic features
        basic_features = [
            *sorted(numbers),
            sum(numbers),
            sum(1 for n in numbers if n % 2),
            sum(1 for i in range(len(numbers)-1) if numbers[i+1] - numbers[i] == 1),
            numbers[-1] - numbers[0],
            np.mean(numbers),
            np.std(numbers)
        ]
        
        # Statistical features (Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©)
        freq_counter = Counter(list(chain.from_iterable(df['numbers'])))
        statistical_features = [
            np.mean([freq_counter.get(n, 0) for n in numbers]),
            np.std([freq_counter.get(n, 0) for n in numbers]),
            0  # Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³Ø­Ø¨ ØªØ§Ù„ÙŠ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        ]
        
        # Pattern features
        pattern_features = [
            len(set([n % 10 for n in numbers])),
            sum(1 for n in numbers if self._is_prime(n)),
            self._calculate_balance(numbers)
        ]
        
        return np.array(basic_features + statistical_features + pattern_features)
    
    def _is_prime(self, n: int) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ù‚Ù… Ø£ÙˆÙ„ÙŠØ§Ù‹"""
        if n < 2:
            return False
        for i in range(2, int(np.sqrt(n)) + 1):
            if n % i == 0:
                return False
        return True
    
    def _calculate_balance(self, numbers: List[int]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªÙˆØ§Ø²Ù†"""
        if len(numbers) < 2:
            return 1.0
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†ØµÙÙŠÙ†
        first_half = sum(1 for n in numbers if n <= 16)
        second_half = len(numbers) - first_half
        balance = 1 - abs(first_half - second_half) / len(numbers)
        
        return balance
    
    def _save_model(self, model_name: str):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ"""
        import os
        os.makedirs(Config.MODELS_DIR, exist_ok=True)
        
        model_path = os.path.join(Config.MODELS_DIR, f'{model_name}.pkl')
        scaler_path = os.path.join(Config.MODELS_DIR, f'{model_name}_scaler.pkl')
        
        joblib.dump(self.models[model_name], model_path)
        joblib.dump(self.scalers[model_name], scaler_path)
    
    def load_model(self, model_name: str):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸"""
        import os
        
        model_path = os.path.join(Config.MODELS_DIR, f'{model_name}.pkl')
        scaler_path = os.path.join(Config.MODELS_DIR, f'{model_name}_scaler.pkl')
        
        if os.path.exists(model_path) and os.path.exists(scaler_path):
            self.models[model_name] = joblib.load(model_path)
            self.scalers[model_name] = joblib.load(scaler_path)
            self.is_trained = True
            logger.logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}")
        else:
            raise FileNotFoundError(f"Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©")
    
    def ensemble_predict(self, current_numbers: List[int], df: pd.DataFrame, 
                        top_n: int = 10) -> List[Tuple[int, float]]:
        """ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ensemble Ù…Ù† Ø¹Ø¯Ø© Ù†Ù…Ø§Ø°Ø¬"""
        all_predictions = []
        
        for model_name in self.models:
            try:
                predictions = self.predict(current_numbers, df, top_n=20, model_name=model_name)
                all_predictions.append(predictions)
            except Exception as e:
                logger.logger.warning(f"ÙØ´Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}: {e}")
                continue
        
        if not all_predictions:
            return []
        
        # Ø¯Ù…Ø¬ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª (Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª)
        combined_scores = Counter()
        
        for predictions in all_predictions:
            for num, prob in predictions:
                combined_scores[num] += prob
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù…ØªÙˆØ³Ø·
        for num in combined_scores:
            combined_scores[num] /= len(all_predictions)
        
        # ØªØ±ØªÙŠØ¨ ÙˆØªØ±Ø´ÙŠØ­
        final_predictions = [(num, score) for num, score in combined_scores.most_common(top_n)]
        
        return final_predictions

class RecommendationEngine:
    """Ù†Ø¸Ø§Ù… ØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªØ¹Ù„Ù… ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    
    def __init__(self):
        self.user_profiles = {}
        self.collaborative_matrix = None
        
    def learn_preferences(self, user_id: str, selected_tickets: List[List[int]], 
                         rejected_tickets: List[List[int]] = None):
        """ØªØ¹Ù„Ù… ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        profile = {
            'selected_patterns': self._extract_patterns(selected_tickets),
            'preferred_numbers': self._get_common_numbers(selected_tickets),
            'avoided_numbers': self._get_common_numbers(rejected_tickets) if rejected_tickets else set(),
            'sum_preference': self._get_sum_preference(selected_tickets),
            'odd_even_preference': self._get_odd_even_preference(selected_tickets),
            'learning_strength': min(1.0, len(selected_tickets) / 10)  # Ù‚ÙˆØ© Ø§Ù„ØªØ¹Ù„Ù…
        }
        
        self.user_profiles[user_id] = profile
        
        logger.logger.info(f"ğŸ¯ ØªØ¹Ù„Ù… ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}", extra={
            'selected_tickets': len(selected_tickets),
            'preferred_numbers_count': len(profile['preferred_numbers']),
            'learning_strength': profile['learning_strength']
        })
    
    def recommend(self, user_id: str, base_tickets: List[List[int]], 
                 count: int = 5) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù…Ø®ØµØµØ©"""
        if user_id not in self.user_profiles:
            return base_tickets[:count]
        
        profile = self.user_profiles[user_id]
        recommendations = []
        
        for base_ticket in base_tickets[:10]:  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆÙ„ 10 ØªØ°Ø§ÙƒØ± ÙƒÙ‚Ø§Ø¹Ø¯Ø©
            customized = self._customize_ticket(base_ticket, profile)
            if customized and customized not in recommendations:
                recommendations.append(customized)
                if len(recommendations) >= count:
                    break
        
        return recommendations
    
    def _extract_patterns(self, tickets: List[List[int]]) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ù…Ù† Ø§Ù„ØªØ°Ø§ÙƒØ±"""
        if not tickets:
            return {}
        
        patterns = {
            'consecutive_range': [],
            'shadow_range': [],
            'sum_range': [],
            'odd_range': []
        }
        
        for ticket in tickets:
            patterns['consecutive_range'].append(
                sum(1 for i in range(len(ticket)-1) if ticket[i+1] - ticket[i] == 1)
            )
            patterns['shadow_range'].append(
                sum(1 for c in Counter([n % 10 for n in ticket]).values() if c > 1)
            )
            patterns['sum_range'].append(sum(ticket))
            patterns['odd_range'].append(sum(1 for n in ticket if n % 2))
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· ÙˆØ§Ù„Ù†Ø·Ø§Ù‚
        for key in patterns:
            if patterns[key]:
                patterns[key] = {
                    'min': min(patterns[key]),
                    'max': max(patterns[key]),
                    'avg': np.mean(patterns[key])
                }
            else:
                patterns[key] = {'min': 0, 'max': 0, 'avg': 0}
        
        return patterns
    
    def _get_common_numbers(self, tickets: List[List[int]]) -> Set[int]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©"""
        if not tickets:
            return set()
        
        counter = Counter()
        for ticket in tickets:
            counter.update(ticket)
        
        # Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªÙŠ ØªØ¸Ù‡Ø± ÙÙŠ Ø£ÙƒØ«Ø± Ù…Ù† 30% Ù…Ù† Ø§Ù„ØªØ°Ø§ÙƒØ±
        threshold = len(tickets) * 0.3
        return {num for num, count in counter.items() if count >= threshold}
    
    def _get_sum_preference(self, tickets: List[List[int]]) -> Dict:
        """ØªØ­Ø¯ÙŠØ¯ ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹"""
        if not tickets:
            return {'min': 20, 'max': 200, 'avg': 100}
        
        sums = [sum(t) for t in tickets]
        return {
            'min': min(sums),
            'max': max(sums),
            'avg': np.mean(sums),
            'std': np.std(sums)
        }
    
    def _get_odd_even_preference(self, tickets: List[List[int]]) -> Dict:
        """ØªØ­Ø¯ÙŠØ¯ ØªÙØ¶ÙŠÙ„ Ø§Ù„ÙØ±Ø¯ÙŠ/Ø§Ù„Ø²ÙˆØ¬ÙŠ"""
        if not tickets:
            return {'min_odd': 0, 'max_odd': 6, 'avg_odd': 3}
        
        odd_counts = [sum(1 for n in t if n % 2) for t in tickets]
        return {
            'min_odd': min(odd_counts),
            'max_odd': max(odd_counts),
            'avg_odd': np.mean(odd_counts),
            'preferred_odd': int(np.round(np.mean(odd_counts)))
        }
    
    def _customize_ticket(self, base_ticket: List[int], profile: Dict) -> List[int]:
        """ØªØ®ØµÙŠØµ Ø§Ù„ØªØ°ÙƒØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª"""
        # Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„ØªØ°ÙƒØ±Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        ticket = base_ticket.copy()
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
        preferred = profile['preferred_numbers']
        avoided = profile['avoided_numbers']
        
        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø±ÙÙˆØ¶Ø© Ø¨Ø§Ù„Ù…ÙØ¶Ù„Ø© Ø¥Ù† Ø£Ù…ÙƒÙ†
        for i in range(len(ticket)):
            if ticket[i] in avoided and preferred:
                # Ø§Ø®ØªÙŠØ§Ø± Ø±Ù‚Ù… Ù…ÙØ¶Ù„ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„ØªØ°ÙƒØ±Ø©
                for pref_num in preferred:
                    if pref_num not in ticket:
                        ticket[i] = pref_num
                        break
        
        # Ø¶Ø¨Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ÙØ±Ø¯ÙŠØ©
        target_odd = profile['odd_even_preference']['preferred_odd']
        current_odd = sum(1 for n in ticket if n % 2)
        
        if current_odd > target_odd:
            # ØªØ­ÙˆÙŠÙ„ Ø¨Ø¹Ø¶ Ø§Ù„ÙØ±Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙˆØ¬ÙŠ
            odd_indices = [i for i, n in enumerate(ticket) if n % 2]
            changes_needed = current_odd - target_odd
            
            for i in odd_indices[:changes_needed]:
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ù‚Ø±Ø¨ Ø±Ù‚Ù… Ø²ÙˆØ¬ÙŠ
                ticket[i] = ticket[i] + 1 if ticket[i] < 32 else ticket[i] - 1
        
        elif current_odd < target_odd:
            # ØªØ­ÙˆÙŠÙ„ Ø¨Ø¹Ø¶ Ø§Ù„Ø²ÙˆØ¬ÙŠ Ø¥Ù„Ù‰ ÙØ±Ø¯ÙŠ
            even_indices = [i for i, n in enumerate(ticket) if n % 2 == 0]
            changes_needed = target_odd - current_odd
            
            for i in even_indices[:changes_needed]:
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ù‚Ø±Ø¨ Ø±Ù‚Ù… ÙØ±Ø¯ÙŠ
                ticket[i] = ticket[i] + 1 if ticket[i] < 32 else ticket[i] - 1
        
        # Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹
        sum_pref = profile['sum_preference']
        current_sum = sum(ticket)
        target_sum = int(sum_pref['avg'])
        
        if abs(current_sum - target_sum) > 10:
            # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù„Ù„Ø§Ù‚ØªØ±Ø§Ø¨ Ù…Ù† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
            diff = target_sum - current_sum
            adjustment_per_num = diff // len(ticket)
            
            if abs(adjustment_per_num) > 0:
                for i in range(len(ticket)):
                    new_val = ticket[i] + adjustment_per_num
                    if 1 <= new_val <= 32:
                        ticket[i] = new_val
        
        return sorted(ticket)