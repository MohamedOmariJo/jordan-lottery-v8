"""
=============================================================================
ğŸ“Š Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø±ÙŠØ§Ø¶ÙŠØ©
=============================================================================
"""

import pandas as pd
import numpy as np
from collections import Counter, defaultdict
from itertools import chain
from typing import List, Dict, Tuple, Optional, Set
from scipy.stats import poisson, norm
import warnings
warnings.filterwarnings('ignore')

from config.settings import Config
from utils.logger import logger

class AdvancedAnalyzer:
    """Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø±ÙŠØ§Ø¶ÙŠØ©"""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.total_draws = len(df)
        self.all_nums = list(chain.from_iterable(df['numbers']))
        self.freq = Counter(self.all_nums)
        
        # Ø¥ØµÙ„Ø§Ø­ Poisson: Ø­Ø³Ø§Ø¨ Lambda Ø§Ù„ØµØ­ÙŠØ­
        self._initialize_poisson_correction()
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
        self._classify_numbers()
        
        # Ø¢Ø®Ø± Ø³Ø­Ø¨
        self.last_draw = set(df.iloc[-1]['numbers']) if not df.empty else set()
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        self._build_advanced_markov()
        self._analyze_poisson_precise()
        self._analyze_patterns()
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        logger.logger.info("ğŸ” ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…", extra={
            'total_draws': self.total_draws,
            'unique_numbers': len(self.freq),
            'classification': {
                'hot_count': len(self.hot),
                'cold_count': len(self.cold),
                'neutral_count': len(self.neutral)
            }
        })
    
    def _initialize_poisson_correction(self):
        """Ø¥ØµÙ„Ø§Ø­ Ø­Ø³Ø§Ø¨ Lambda Ù„ØªØ­Ù„ÙŠÙ„ Poisson"""
        # âŒ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø£ØµÙ„ÙŠ: expected_per_draw = 6/32 = 0.1875 (Ù†Ø³Ø¨Ø©)
        # âŒ Ø«Ù…: lambda_param = 0.1875 * total_draws (Ø®Ø·Ø£)
        
        # âœ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­: Ø­Ø³Ø§Ø¨ ØµØ­ÙŠØ­ Ù„Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ
        # Ù„ÙƒÙ„ Ø³Ø­Ø¨: Ø§Ø­ØªÙ…Ø§Ù„ Ø¸Ù‡ÙˆØ± Ø±Ù‚Ù… Ù…Ø¹ÙŠÙ† = 6/32 = 0.1875
        # Ø§Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ Ù„Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø¸Ù‡ÙˆØ± Ø§Ù„Ø±Ù‚Ù… ÙÙŠ ÙƒÙ„ Ø§Ù„Ø³Ø­ÙˆØ¨Ø§Øª:
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©:
        # 1. Ø§Ø­ØªÙ…Ø§Ù„ Ø¸Ù‡ÙˆØ± Ø§Ù„Ø±Ù‚Ù… ÙÙŠ Ø³Ø­Ø¨ ÙˆØ§Ø­Ø¯ = 6/32
        # 2. Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª = total_draws
        # 3. ØªÙˆÙ‚Ø¹ Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø§Ù„Ø¸Ù‡ÙˆØ± = (6/32) * total_draws
        
        # Ù„ÙƒÙ† Poisson ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ rate (Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ø­Ø¯Ø§Ø« ÙÙŠ ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ©)
        # ÙÙŠ Ø­Ø§Ù„ØªÙ†Ø§: rate = (6/32) * total_draws (Ù„ÙƒÙ„ Ø§Ù„Ø±Ù‚Ù…)
        
        self.poisson_rate = (Config.DEFAULT_TICKET_SIZE / Config.MAX_NUMBER) * self.total_draws
        
        logger.logger.info("ğŸ“ Ø¥ØµÙ„Ø§Ø­ Ø­Ø³Ø§Ø¨ Poisson", extra={
            'old_calculation': 'poisson.pmf(k, k) [Ø®Ø·Ø£]',
            'new_calculation': f'poisson.pmf(k, {self.poisson_rate:.4f})',
            'poisson_rate': round(self.poisson_rate, 4),
            'ticket_size': Config.DEFAULT_TICKET_SIZE,
            'max_number': Config.MAX_NUMBER,
            'total_draws': self.total_draws
        })
    
    def _classify_numbers(self):
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¥Ù„Ù‰ Ø³Ø§Ø®Ù†/Ø¨Ø§Ø±Ø¯/Ù…Ø­Ø§ÙŠØ¯"""
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù„Ù„ØªØµÙ†ÙŠÙ
        frequencies = list(self.freq.values())
        mean_freq = np.mean(frequencies)
        std_freq = np.std(frequencies)
        
        self.hot = set()
        self.cold = set()
        self.neutral = set()
        
        for num in range(1, 33):
            freq = self.freq.get(num, 0)
            z_score = (freq - mean_freq) / std_freq if std_freq > 0 else 0
            
            if z_score > 1.0:  # Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÙŠØ§Ø±ÙŠ ÙˆØ§Ø­Ø¯
                self.hot.add(num)
            elif z_score < -1.0:  # Ø£Ù‚Ù„ Ù…Ù† Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÙŠØ§Ø±ÙŠ ÙˆØ§Ø­Ø¯
                self.cold.add(num)
            else:
                self.neutral.add(num)
    
    def _build_advanced_markov(self):
        """Ø¨Ù†Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© Ù…Ø§Ø±ÙƒÙˆÙ Ù…Ù† Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        # Markov Order 1 Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self.markov_1 = {}
        
        # Markov Order 2 Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self.markov_2 = {}
        
        # Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ ÙÙ‚Ø· Ø¨Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
        markov_1_counts = defaultdict(Counter)
        markov_2_counts = defaultdict(Counter)
        
        for i in range(len(self.df) - 1):
            current = self.df.iloc[i]['numbers']
            next_draw = self.df.iloc[i + 1]['numbers']
            
            # Order 1
            for num in current:
                markov_1_counts[num].update(next_draw)
            
            # Order 2 (pairs)
            for j in range(len(current) - 1):
                pair = (current[j], current[j + 1])
                markov_2_counts[pair].update(next_draw)
        
        # ØªØµÙÙŠØ© Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„Ø§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø©
        for num, counter in markov_1_counts.items():
            total = sum(counter.values())
            if total >= Config.MARKOV_MIN_OCCURRENCES:
                self.markov_1[num] = {
                    num: count/total 
                    for num, count in counter.most_common(10)  # Ø­ÙØ¸ Ø£ÙØ¶Ù„ 10 ÙÙ‚Ø·
                }
        
        for pair, counter in markov_2_counts.items():
            total = sum(counter.values())
            if total >= Config.MARKOV_MIN_OCCURRENCES:
                self.markov_2[pair] = {
                    num: count/total 
                    for num, count in counter.most_common(8)  # Ø­ÙØ¸ Ø£ÙØ¶Ù„ 8 ÙÙ‚Ø·
                }
        
        logger.logger.info("ğŸ”— Ø¨Ù†Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© Ù…Ø§Ø±ÙƒÙˆÙ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©", extra={
            'markov_1_states': len(self.markov_1),
            'markov_2_states': len(self.markov_2),
            'min_occurrences': Config.MARKOV_MIN_OCCURRENCES
        })
    
    def _analyze_poisson_precise(self):
        """ØªØ­Ù„ÙŠÙ„ Ø¨ÙˆØ§Ø³ÙˆÙ† Ø¯Ù‚ÙŠÙ‚ Ù…Ø¹ Z-Score ØµØ­ÙŠØ­"""
        self.poisson_data = []
        
        for num in range(Config.MIN_NUMBER, Config.MAX_NUMBER + 1):
            actual_count = self.freq.get(num, 0)
            
            # âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Lambda Ø§Ù„Ù…ØµØ­Ø­
            poisson_prob = poisson.pmf(actual_count, self.poisson_rate)
            
            # Ø­Ø³Ø§Ø¨ Z-Score ØµØ­ÙŠØ­
            # Ù„Ù„Ù€ Poisson: Î¼ = Î», Ïƒ = âˆšÎ»
            std_dev = np.sqrt(self.poisson_rate)
            z_score = (actual_count - self.poisson_rate) / std_dev if std_dev > 0 else 0
            
            # Ø­Ø³Ø§Ø¨ Ù‚ÙŠÙ…Ø© P (Ø§Ù„Ø¯Ù„Ø§Ù„Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©)
            p_value = 2 * (1 - norm.cdf(abs(z_score))) if std_dev > 0 else 1
            
            # Ø§Ù„Ø´Ø°ÙˆØ° = Ù…Ø¯Ù‰ Ø¨Ø¹Ø¯ Ø§Ù„Ø±Ù‚Ù… Ø¹Ù† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
            anomaly_score = abs(z_score)
            
            # Ø­Ø³Ø§Ø¨ Ø¢Ø®Ø± Ø¸Ù‡ÙˆØ±
            appearances = [i for i, nums in enumerate(self.df['numbers']) if num in nums]
            last_seen = self.total_draws - 1 - appearances[-1] if appearances else self.total_draws
            
            # Ù…ØªÙˆØ³Ø· Ø§Ù„ÙØ¬ÙˆØ©
            if len(appearances) > 1:
                gaps = np.diff(appearances)
                avg_gap = np.mean(gaps)
                gap_std = np.std(gaps)
            else:
                avg_gap = self.total_draws
                gap_std = 0
            
            self.poisson_data.append({
                'number': num,
                'frequency': actual_count,
                'expected': round(self.poisson_rate, 2),
                'last_seen': last_seen,
                'avg_gap': round(avg_gap, 2),
                'gap_std': round(gap_std, 2),
                'z_score': round(z_score, 3),
                'p_value': round(p_value, 4),
                'anomaly_score': round(anomaly_score, 3),
                'is_significant': p_value < 0.05,
                'status': 'hot' if num in self.hot else 'cold' if num in self.cold else 'neutral',
                'classification': self._classify_anomaly(z_score, p_value)
            })
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø´Ø°ÙˆØ°Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
            if abs(z_score) > 2.5:
                logger.log_anomaly(num, z_score, self.poisson_rate, actual_count)
    
    def _classify_anomaly(self, z_score: float, p_value: float) -> str:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ"""
        if abs(z_score) > 3 and p_value < 0.01:
            return "extreme_anomaly"
        elif abs(z_score) > 2.5 and p_value < 0.05:
            return "significant_anomaly"
        elif abs(z_score) > 2:
            return "moderate_anomaly"
        elif abs(z_score) > 1.5:
            return "mild_anomaly"
        else:
            return "normal"
    
    def _analyze_patterns(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        self.patterns = {
            'consecutive_freq': defaultdict(int),
            'shadow_freq': defaultdict(int),
            'sum_distribution': [],
            'odd_even_ratio': [],
            'decade_distribution': defaultdict(int),  # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ø´Ø±Ø§Øª
            'prime_distribution': defaultdict(int),   # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
            'cluster_analysis': self._analyze_clusters()
        }
        
        for nums in self.df['numbers']:
            # Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ§Øª
            consec = sum(1 for i in range(len(nums) - 1) if nums[i + 1] - nums[i] == 1)
            self.patterns['consecutive_freq'][consec] += 1
            
            # Ø§Ù„Ø¸Ù„Ø§Ù„ (Ø£Ø±Ù‚Ø§Ù… Ø¨Ù†ÙØ³ Ø®Ø§Ù†Ø© Ø§Ù„Ø¢Ø­Ø§Ø¯)
            shadows = sum(1 for c in Counter([n % 10 for n in nums]).values() if c > 1)
            self.patterns['shadow_freq'][shadows] += 1
            
            # Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹
            self.patterns['sum_distribution'].append(sum(nums))
            
            # Ù†Ø³Ø¨Ø© Ø§Ù„ÙØ±Ø¯ÙŠ/Ø§Ù„Ø²ÙˆØ¬ÙŠ
            odd_count = sum(1 for n in nums if n % 2)
            self.patterns['odd_even_ratio'].append(odd_count)
            
            # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ø´Ø±Ø§Øª
            for num in nums:
                decade = (num - 1) // 10  # 0-2: 1-10, 11-20, 21-30
                self.patterns['decade_distribution'][decade] += 1
            
            # Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
            prime_count = sum(1 for n in nums if self._is_prime(n))
            self.patterns['prime_distribution'][prime_count] += 1
    
    def _analyze_clusters(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§Ù‚ÙŠØ¯ ÙˆØ§Ù„ØªØ¬Ù…Ø¹Ø§Øª"""
        clusters = []
        for nums in self.df['numbers']:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ¬Ù…Ø¹Ø§Øª (Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø£Ø±Ù‚Ø§Ù… Ù…ØªÙ‚Ø§Ø±Ø¨Ø©)
            sorted_nums = sorted(nums)
            current_cluster = [sorted_nums[0]]
            
            for i in range(1, len(sorted_nums)):
                if sorted_nums[i] - sorted_nums[i-1] <= 3:  # ÙØ±Ù‚ 3 Ø£Ùˆ Ø£Ù‚Ù„
                    current_cluster.append(sorted_nums[i])
                else:
                    if len(current_cluster) >= 3:
                        clusters.append(current_cluster)
                    current_cluster = [sorted_nums[i]]
            
            if len(current_cluster) >= 3:
                clusters.append(current_cluster)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§Ù‚ÙŠØ¯
        cluster_stats = {
            'total_clusters': len(clusters),
            'avg_cluster_size': np.mean([len(c) for c in clusters]) if clusters else 0,
            'max_cluster_size': max([len(c) for c in clusters]) if clusters else 0,
            'common_clusters': Counter([tuple(c) for c in clusters]).most_common(5)
        }
        
        return cluster_stats
    
    def _is_prime(self, n: int) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ù‚Ù… Ø£ÙˆÙ„ÙŠØ§Ù‹"""
        if n < 2:
            return False
        for i in range(2, int(np.sqrt(n)) + 1):
            if n % i == 0:
                return False
        return True
    
    def get_markov_prediction(self, last_numbers: List[int], top_n: int = 8) -> List[Tuple[int, float]]:
        """ØªÙˆÙ‚Ø¹ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Markov Ù…Ø¹ Ø£ÙˆØ²Ø§Ù† Ø°ÙƒÙŠØ©"""
        candidates = Counter()
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Order 1 Ù…Ø¹ Ø£ÙˆØ²Ø§Ù†
        for num in last_numbers:
            if num in self.markov_1:
                for next_num, prob in self.markov_1[num].items():
                    candidates[next_num] += prob * 1.0  # ÙˆØ²Ù† Order 1
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Order 2 (Ø£ÙˆØ²Ø§Ù† Ø£Ø¹Ù„Ù‰)
        for i in range(len(last_numbers) - 1):
            pair = (last_numbers[i], last_numbers[i + 1])
            if pair in self.markov_2:
                for next_num, prob in self.markov_2[pair].items():
                    candidates[next_num] += prob * 2.0  # ÙˆØ²Ù† Ù…Ø¶Ø§Ø¹Ù Ù„Ù€ Order 2
        
        # Ø¥Ø¶Ø§ÙØ© Ø¹Ø§Ù…Ù„ Ø§Ù„Ø´Ø¹Ø¨ÙŠØ© (Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ)
        for num, freq in self.freq.most_common(20):
            if num not in last_numbers:
                popularity_factor = freq / max(self.freq.values())
                candidates[num] += popularity_factor * 0.5  # ÙˆØ²Ù† Ù…ØªÙˆØ³Ø·
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        total = sum(candidates.values())
        if total == 0:
            return []
        
        probabilities = [(num, count / total) for num, count in candidates.most_common(top_n)]
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª
        logger.log_prediction('markov_chain', 0.0, probabilities[0][1] if probabilities else 0, 
                            ['markov_1', 'markov_2', 'popularity'])
        
        return probabilities
    
    def get_ticket_analysis(self, ticket: List[int]) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„ØªØ°ÙƒØ±Ø© Ù…Ø¹ÙŠÙ†Ø©"""
        nums = sorted(ticket)
        
        analysis = {
            'basic': {
                'sum': sum(nums),
                'odd': sum(1 for n in nums if n % 2),
                'even': sum(1 for n in nums if n % 2 == 0),
                'consecutive': sum(1 for i in range(len(nums) - 1) if nums[i + 1] - nums[i] == 1),
                'shadows': sum(1 for c in Counter([n % 10 for n in nums]).values() if c > 1),
                'range_width': nums[-1] - nums[0],
                'avg_spacing': np.mean([nums[i+1] - nums[i] for i in range(len(nums)-1)]) if len(nums) > 1 else 0
            },
            'classification': {
                'hot_count': len(set(nums) & self.hot),
                'cold_count': len(set(nums) & self.cold),
                'neutral_count': len(set(nums) & self.neutral),
                'last_match': len(set(nums) & self.last_draw)
            },
            'statistical': {
                'avg_frequency': round(np.mean([self.freq[n] for n in nums]), 2),
                'freq_std': round(np.std([self.freq[n] for n in nums]), 2),
                'diversity_score': round(len(set([self.freq[n] for n in nums])) / len(nums), 3),
                'balance_score': self._calculate_balance_score(nums)
            },
            'advanced': {
                'prime_count': sum(1 for n in nums if self._is_prime(n)),
                'decade_distribution': self._get_decade_distribution(nums),
                'cluster_score': self._calculate_cluster_score(nums),
                'pattern_complexity': self._calculate_pattern_complexity(nums)
            }
        }
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©
        analysis['quality_score'] = self._calculate_quality_score(analysis)
        
        return analysis
    
    def _calculate_balance_score(self, nums: List[int]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ Ø§Ù„ØªØ°ÙƒØ±Ø©"""
        if len(nums) < 2:
            return 1.0
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ø´Ø±Ø§Øª
        decades = [(n - 1) // 10 for n in nums]
        decade_balance = len(set(decades)) / 3  # 3 Ø¹Ù‚ÙˆØ¯
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†ØµÙÙŠÙ† (1-16, 17-32)
        first_half = sum(1 for n in nums if n <= 16)
        second_half = len(nums) - first_half
        half_balance = 1 - abs(first_half - second_half) / len(nums)
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø±Ø¨Ø§Ø¹
        quarters = [0, 0, 0, 0]
        for n in nums:
            quarter = (n - 1) // 8
            quarters[quarter] += 1
        quarter_balance = len([q for q in quarters if q > 0]) / 4
        
        return round((decade_balance + half_balance + quarter_balance) / 3, 3)
    
    def _get_decade_distribution(self, nums: List[int]) -> Dict:
        """ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù‚ÙˆØ¯"""
        distribution = {0: 0, 1: 0, 2: 0, 3: 0}  # 1-10, 11-20, 21-30, 31-32
        for n in nums:
            decade = (n - 1) // 10
            distribution[min(decade, 3)] += 1
        return distribution
    
    def _calculate_cluster_score(self, nums: List[int]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ¬Ù…Ø¹"""
        if len(nums) < 3:
            return 0.0
        
        sorted_nums = sorted(nums)
        clusters = []
        current_cluster = [sorted_nums[0]]
        
        for i in range(1, len(sorted_nums)):
            if sorted_nums[i] - sorted_nums[i-1] <= 3:
                current_cluster.append(sorted_nums[i])
            else:
                if len(current_cluster) >= 2:
                    clusters.append(current_cluster)
                current_cluster = [sorted_nums[i]]
        
        if len(current_cluster) >= 2:
            clusters.append(current_cluster)
        
        if not clusters:
            return 0.0
        
        # Ø¯Ø±Ø¬Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ ÙˆØ­Ø¬Ù… Ø§Ù„Ø¹Ù†Ø§Ù‚ÙŠØ¯
        total_clustered = sum(len(c) for c in clusters)
        cluster_score = total_clustered / len(nums)
        
        # Ø¹Ù‚ÙˆØ¨Ø© Ù„Ù„Ø¹Ù†Ø§Ù‚ÙŠØ¯ Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
        max_cluster_size = max(len(c) for c in clusters) if clusters else 0
        if max_cluster_size > 4:
            cluster_score *= 0.7
        
        return round(cluster_score, 3)
    
    def _calculate_pattern_complexity(self, nums: List[int]) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Ù…Ø·"""
        features = [
            len(set(nums)),  # Ø§Ù„ØªÙØ±Ø¯
            len(set([n % 10 for n in nums])),  # ØªÙ†ÙˆØ¹ Ø§Ù„Ø¢Ø­Ø§Ø¯
            sum(1 for i in range(len(nums)-1) if nums[i+1] - nums[i] == 1),  # Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ§Øª
            len([n for n in nums if self._is_prime(n)]),  # Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
            nums[-1] - nums[0]  # Ø¹Ø±Ø¶ Ø§Ù„Ù†Ø·Ø§Ù‚
        ]
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        normalized = [f / len(nums) for f in features]
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§ (Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯)
        entropy = -sum(p * np.log2(p) if p > 0 else 0 for p in normalized)
        
        return round(entropy / np.log2(len(features)), 3)
    
    def _calculate_quality_score(self, analysis: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø© Ù„Ù„ØªØ°ÙƒØ±Ø©"""
        weights = {
            'balance': 0.25,
            'diversity': 0.20,
            'pattern': 0.15,
            'statistical': 0.20,
            'historical': 0.20
        }
        
        scores = {
            'balance': analysis['statistical']['balance_score'],
            'diversity': analysis['statistical']['diversity_score'],
            'pattern': analysis['advanced']['pattern_complexity'],
            'statistical': min(1.0, analysis['statistical']['avg_frequency'] / 10),  # ØªØ·Ø¨ÙŠØ¹
            'historical': analysis['classification']['hot_count'] / len(analysis['basic'])
        }
        
        quality_score = sum(scores[key] * weights[key] for key in weights)
        return round(quality_score * 10, 2)  # ØªØ­ÙˆÙŠÙ„ Ù„Ù…Ù‚ÙŠØ§Ø³ 0-10