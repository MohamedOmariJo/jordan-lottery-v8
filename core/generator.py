"""
=============================================================================
ğŸ° Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªØ°Ø§ÙƒØ± Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
=============================================================================
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

from config.settings import Config
from utils.logger import logger
from utils.performance import PerformanceBenchmark
from core.analyzer import AdvancedAnalyzer

class SmartGenerator:
    """Ù…ÙˆÙ„Ø¯ ØªØ°Ø§ÙƒØ± Ø°ÙƒÙŠ Ù…Ø¹ ÙÙ„Ø§ØªØ± Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØªØ­Ø³ÙŠÙ†Ø§Øª Ø£Ø¯Ø§Ø¡"""
    
    def __init__(self, analyzer: AdvancedAnalyzer):
        self.analyzer = analyzer
        self.benchmark = PerformanceBenchmark()
        self.cache = {}
        
    def generate_tickets(
        self,
        count: int,
        size: int = 6,
        constraints: Optional[Dict] = None,
        use_cache: bool = True
    ) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ°Ø§ÙƒØ± Ù…Ø¹ ÙÙ„Ø§ØªØ± Ù…Ø­Ø³Ù†Ø© ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… cache"""
        
        if constraints is None:
            constraints = {}
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Cache Ø£ÙˆÙ„Ø§Ù‹
        cache_key = self._generate_cache_key(count, size, constraints)
        if use_cache and cache_key in self.cache:
            logger.logger.info(f"ğŸ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Cache Ù„Ù„ØªÙˆÙ„ÙŠØ¯ - Ù…ÙØªØ§Ø­: {cache_key[:50]}...")
            return self.cache[cache_key].copy()
        
        op_id = logger.start_operation('ticket_generation', {
            'count': count,
            'size': size,
            'constraints': constraints
        })
        
        try:
            with self.benchmark.monitor_operation('generation'):
                # Ø¥Ø¹Ø¯Ø§Ø¯ Pool Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
                pool = self._prepare_number_pool(constraints)
                
                if len(pool) < size:
                    error_msg = f"âŒ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ØªØ§Ø­Ø© ({len(pool)}) Ø£Ù‚Ù„ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØªØ°ÙƒØ±Ø© ({size})"
                    logger.logger.error(error_msg)
                    raise ValueError(error_msg)
                
                # Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø«Ù„Ù‰
                if count <= 10:
                    tickets = self._generate_small_batch(pool, size, count, constraints)
                elif count <= 100:
                    tickets = self._generate_medium_batch(pool, size, count, constraints)
                else:
                    tickets = self._generate_large_batch(pool, size, count, constraints)
                
                # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
                if constraints:
                    tickets = self._apply_advanced_filters(tickets, constraints)
                
                # Ø§Ù„Ø­Ø¯ Ù…Ù† Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
                tickets = tickets[:count]
                
                # ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Cache
                if use_cache and len(tickets) > 0:
                    self.cache[cache_key] = tickets.copy()
                    # ØªÙ†Ø¸ÙŠÙ Cache Ø§Ù„Ù‚Ø¯ÙŠÙ…
                    self._clean_cache()
                
                logger.end_operation(op_id, 'completed', {
                    'generated_count': len(tickets),
                    'success_rate': round(len(tickets) / count * 100, 2),
                    'cache_used': use_cache,
                    'cache_key': cache_key[:30]
                })
                
                return tickets
                
        except Exception as e:
            logger.end_operation(op_id, 'failed', {'error': str(e)})
            raise
    
    def _prepare_number_pool(self, constraints: Dict) -> List[int]:
        """ØªØ­Ø¶ÙŠØ± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯"""
        pool = list(range(Config.MIN_NUMBER, Config.MAX_NUMBER + 1))
        
        if 'exclude' in constraints:
            exclude_set = set(constraints['exclude'])
            pool = [n for n in pool if n not in exclude_set]
        
        # Ø¥Ø¶Ø§ÙØ© ØªØ­Ø³ÙŠÙ†: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø°Ø§Øª Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…Ù†Ø®ÙØ¶ Ø¬Ø¯Ø§Ù‹
        if constraints.get('filter_low_freq', False):
            avg_freq = np.mean(list(self.analyzer.freq.values()))
            pool = [n for n in pool if self.analyzer.freq.get(n, 0) >= avg_freq * 0.5]
        
        return pool
    
    def _generate_small_batch(self, pool: List[int], size: int, 
                            count: int, constraints: Dict) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø§Øª ØµØºÙŠØ±Ø© (<= 10)"""
        tickets = []
        attempts = 0
        max_attempts = count * 100
        
        while len(tickets) < count and attempts < max_attempts:
            attempts += 1
            
            # ØªÙˆÙ„ÙŠØ¯ ØªØ°ÙƒØ±Ø©
            ticket = sorted(random.sample(pool, size))
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            if not self._satisfies_basic_constraints(ticket, constraints):
                continue
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
            if not self._satisfies_advanced_constraints(ticket, constraints):
                continue
            
            # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
            ticket_tuple = tuple(ticket)
            if ticket_tuple not in [tuple(t) for t in tickets]:
                tickets.append(ticket)
        
        return tickets
    
    def _generate_medium_batch(self, pool: List[int], size: int, 
                             count: int, constraints: Dict) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø§Øª Ù…ØªÙˆØ³Ø·Ø© (<= 100) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ­Ø³ÙŠÙ†Ø§Øª"""
        tickets_set = set()
        batch_size = min(1000, count * 10)
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            
            for _ in range(min(10, count)):
                future = executor.submit(
                    self._generate_batch_parallel,
                    pool, size, batch_size, constraints
                )
                futures.append(future)
            
            for future in as_completed(futures):
                batch_tickets = future.result()
                for ticket in batch_tickets:
                    if len(tickets_set) >= count:
                        break
                    tickets_set.add(tuple(ticket))
        
        return [list(t) for t in list(tickets_set)[:count]]
    
    def _generate_large_batch(self, pool: List[int], size: int, 
                            count: int, constraints: Dict) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø§Øª ÙƒØ¨ÙŠØ±Ø© (> 100) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… vectorization"""
        tickets_set = set()
        total_generated = 0
        
        while len(tickets_set) < count and total_generated < count * 100:
            # ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø© ÙƒØ¨ÙŠØ±Ø©
            batch_size = min(10000, (count - len(tickets_set)) * 10)
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… numpy Ù„Ù„Ø³Ø±Ø¹Ø©
            batch = np.array([
                np.random.choice(pool, size=size, replace=False)
                for _ in range(batch_size)
            ])
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ø³Ø±Ø¹Ø©
            batch = self._filter_batch_vectorized(batch, constraints)
            
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù†ØªØ§Ø¦Ø¬
            for ticket in batch:
                ticket_tuple = tuple(sorted(ticket))
                tickets_set.add(ticket_tuple)
                if len(tickets_set) >= count:
                    break
            
            total_generated += batch_size
        
        return [list(t) for t in list(tickets_set)[:count]]
    
    def _generate_batch_parallel(self, pool: List[int], size: int, 
                               batch_size: int, constraints: Dict) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø© Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ"""
        batch_tickets = []
        
        for _ in range(batch_size):
            ticket = sorted(random.sample(pool, size))
            
            if (self._satisfies_basic_constraints(ticket, constraints) and 
                self._satisfies_advanced_constraints(ticket, constraints)):
                batch_tickets.append(ticket)
        
        return batch_tickets
    
    def _filter_batch_vectorized(self, batch: np.ndarray, constraints: Dict) -> np.ndarray:
        """ØªØµÙÙŠØ© Ø§Ù„Ø¯ÙØ¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… vectorization"""
        if batch.size == 0:
            return batch
        
        masks = []
        
        # ÙÙ„ØªØ± Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹
        if 'sum_range' in constraints:
            min_sum, max_sum = constraints['sum_range']
            sums = batch.sum(axis=1)
            masks.append((sums >= min_sum) & (sums <= max_sum))
        
        # ÙÙ„ØªØ± Ø§Ù„ÙØ±Ø¯ÙŠ
        if 'odd' in constraints:
            target_odd = constraints['odd']
            odd_counts = np.sum(batch % 2, axis=1)
            masks.append(odd_counts == target_odd)
        
        # ÙÙ„ØªØ± Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ§Øª
        if 'consecutive' in constraints:
            target_consec = constraints['consecutive']
            consec_counts = np.array([
                np.sum(np.diff(row) == 1)
                for row in batch
            ])
            masks.append(consec_counts == target_consec)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù‚Ù†Ø¹Ø©
        if masks:
            combined_mask = np.all(masks, axis=0)
            batch = batch[combined_mask]
        
        return batch
    
    def _satisfies_basic_constraints(self, ticket: List[int], constraints: Dict) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        # Ø§Ù„ÙØ±Ø¯ÙŠ
        if 'odd' in constraints:
            odd_count = sum(1 for n in ticket if n % 2)
            if odd_count != constraints['odd']:
                return False
        
        # Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹
        if 'sum_range' in constraints:
            ticket_sum = sum(ticket)
            min_sum, max_sum = constraints['sum_range']
            if not (min_sum <= ticket_sum <= max_sum):
                return False
        
        # Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø«Ø§Ø¨ØªØ©
        if 'fixed' in constraints:
            fixed_set = set(constraints['fixed'])
            if not fixed_set.issubset(set(ticket)):
                return False
        
        return True
    
    def _satisfies_advanced_constraints(self, ticket: List[int], constraints: Dict) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        # Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ§Øª
        if 'consecutive' in constraints:
            consec_count = sum(1 for i in range(len(ticket)-1) 
                             if ticket[i+1] - ticket[i] == 1)
            if consec_count != constraints['consecutive']:
                return False
        
        # Ø§Ù„Ø¸Ù„Ø§Ù„
        if 'shadows' in constraints:
            shadows_count = sum(1 for c in Counter([n % 10 for n in ticket]).values() 
                              if c > 1)
            if shadows_count != constraints['shadows']:
                return False
        
        # Hot/Cold
        if 'hot_min' in constraints:
            hot_count = len(set(ticket) & self.analyzer.hot)
            if hot_count < constraints['hot_min']:
                return False
        
        if 'cold_max' in constraints:
            cold_count = len(set(ticket) & self.analyzer.cold)
            if cold_count > constraints['cold_max']:
                return False
        
        # Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø¢Ø®Ø± Ø³Ø­Ø¨
        if 'last_match' in constraints:
            match_count = len(set(ticket) & self.analyzer.last_draw)
            if match_count != constraints['last_match']:
                return False
        
        return True
    
    def _apply_advanced_filters(self, tickets: List[List[int]], constraints: Dict) -> List[List[int]]:
        """ØªØ·Ø¨ÙŠÙ‚ ÙÙ„Ø§ØªØ± Ù…ØªÙ‚Ø¯Ù…Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆÙ„ÙŠØ¯"""
        filtered_tickets = []
        
        for ticket in tickets:
            if self._satisfies_advanced_constraints(ticket, constraints):
                filtered_tickets.append(ticket)
        
        return filtered_tickets
    
    def generate_markov_based(self, count: int, size: int = 6) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ°Ø§ÙƒØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Markov"""
        op_id = logger.start_operation('markov_generation', {
            'count': count,
            'size': size
        })
        
        try:
            with self.benchmark.monitor_operation('markov_generation'):
                tickets = []
                last_nums = sorted(list(self.analyzer.last_draw))
                
                for _ in range(count):
                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª
                    predictions = self.analyzer.get_markov_prediction(last_nums, top_n=15)
                    
                    if not predictions:
                        # fallback Ø¥Ù„Ù‰ Ø¹Ø´ÙˆØ§Ø¦ÙŠ
                        ticket = sorted(random.sample(range(1, 33), size))
                    else:
                        # Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø¨Ø£ÙˆØ²Ø§Ù†
                        candidates, weights = zip(*predictions)
                        
                        # ØªÙƒÙ…Ù„Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† ÙƒØ§ÙÙŠ
                        while len(candidates) < size * 2:
                            remaining = list(set(range(1, 33)) - set(candidates))
                            candidates = list(candidates) + random.sample(remaining, 
                                                                         min(size * 2 - len(candidates), 
                                                                             len(remaining)))
                        
                        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø£ÙˆØ²Ø§Ù†
                        weights = np.array(weights[:len(candidates)])
                        weights = weights / weights.sum()
                        
                        # Ø§Ø®ØªÙŠØ§Ø± Ø¨Ø£ÙˆØ²Ø§Ù†
                        selected = np.random.choice(
                            candidates,
                            size=size,
                            replace=False,
                            p=weights
                        )
                        ticket = sorted(selected.tolist())
                    
                    if ticket not in tickets:
                        tickets.append(ticket)
                
                logger.end_operation(op_id, 'completed', {
                    'generated_count': len(tickets),
                    'markov_used': len(predictions) > 0 if 'predictions' in locals() else False
                })
                
                return tickets
                
        except Exception as e:
            logger.end_operation(op_id, 'failed', {'error': str(e)})
            raise
    
    def _generate_cache_key(self, count: int, size: int, constraints: Dict) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ Cache ÙØ±ÙŠØ¯"""
        import hashlib
        import json
        
        data = {
            'count': count,
            'size': size,
            'constraints': constraints,
            'analyzer_hash': hash(str(self.analyzer.freq))
        }
        
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def _clean_cache(self):
        """ØªÙ†Ø¸ÙŠÙ Cache Ø§Ù„Ù‚Ø¯ÙŠÙ…"""
        max_cache_size = 100  # Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ØµØ± ÙÙŠ Cache
        
        if len(self.cache) > max_cache_size:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø£Ù‚Ø¯Ù… (Ø¨Ø³ÙŠØ· - ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ø³ØªØ®Ø¯Ù… LRU)
            keys_to_remove = list(self.cache.keys())[:len(self.cache) - max_cache_size]
            for key in keys_to_remove:
                del self.cache[key]
    
    def generate_with_ml(self, count: int, size: int = 6, 
                        model_name: str = 'random_forest') -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ°Ø§ÙƒØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†Ø¨Ø¤Ø§Øª ML"""
        op_id = logger.start_operation('ml_generation', {
            'count': count,
            'size': size,
            'model': model_name
        })
        
        try:
            with self.benchmark.monitor_operation('ml_generation'):
                # Ù‡Ø°Ø§ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ integration Ù…Ø¹ predictor
                # Ù‡Ù†Ø§ Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ·
                tickets = []
                
                for _ in range(count):
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†Ø¨Ø¤Ø§Øª ML ÙƒÙ…Ø±Ø¬Ø¹
                    ticket = self._generate_ml_inspired_ticket(size)
                    if ticket not in tickets:
                        tickets.append(ticket)
                
                logger.end_operation(op_id, 'completed', {
                    'generated_count': len(tickets),
                    'model_used': model_name
                })
                
                return tickets
                
        except Exception as e:
            logger.end_operation(op_id, 'failed', {'error': str(e)})
            raise
    
    def _generate_ml_inspired_ticket(self, size: int) -> List[int]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ°ÙƒØ±Ø© Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† ØªÙ†Ø¨Ø¤Ø§Øª ML"""
        # Ù‡Ø°Ø§ Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ· - ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø³ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ML ÙØ¹Ù„ÙŠ
        pool = list(range(1, 33))
        
        # Ø¥Ø¹Ø·Ø§Ø¡ ÙˆØ²Ù† Ø£Ø¹Ù„Ù‰ Ù„Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø³Ø§Ø®Ù†Ø©
        weights = np.ones(32)
        for num in self.analyzer.hot:
            weights[num-1] = 2.0
        for num in self.analyzer.cold:
            weights[num-1] = 0.5
        
        weights = weights / weights.sum()
        
        ticket = np.random.choice(
            pool,
            size=size,
            replace=False,
            p=weights
        )
        
        return sorted(ticket.tolist())
    
    def get_generation_stats(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯"""
        return {
            'cache_size': len(self.cache),
            'performance_stats': self.benchmark.get_performance_report('generation'),
            'generator_info': {
                'class': self.__class__.__name__,
                'analyzer_initialized': self.analyzer is not None,
                'methods_available': [
                    'generate_tickets',
                    'generate_markov_based',
                    'generate_with_ml'
                ]
            }
        }